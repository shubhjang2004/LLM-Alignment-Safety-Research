{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vcMU3xrS4Tj"
   },
   "outputs": [],
   "source": [
    "# notebooks/2_rlhf_track.ipynb\n",
    "\n",
    "\"\"\"\n",
    "Project 4: Privacy-Preserving Alignment\n",
    "Notebook 2: RLHF Track (All RLHF Variants)\n",
    "\n",
    "Purpose: Train reward model + RLHF models (baseline + DP variants)\n",
    "Optimized: 15K samples, 2 epochs, MAX_LENGTH=224\n",
    "Time: ~2 hours on T4 (reward model + 2 RLHF models)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BrwKfJDWZ1_w",
    "outputId": "fc47cdd1-d3f6-4cea-d03f-487f7bd61d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/423.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m419.8/423.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/254.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.4/254.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets peft trl opacus accelerate  --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Id41_9ZyS6pk",
    "outputId": "d77b8f11-9e67-49e1-f75e-6d3af7f001b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      " Google Drive mounted successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "if os.path.exists('/content/drive/MyDrive'):\n",
    "    print(\" Google Drive mounted successfully!\")\n",
    "else:\n",
    "    print(\"Drive mount failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wFrd40xFZN_8",
    "outputId": "10e5e4d4-898f-4566-c697-77cb79ccc773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CELL 2: Setup\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType\n",
    ")\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import shutil\n",
    "\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cF_8_VHJZOC8",
    "outputId": "54f05b01-9f5a-4d8e-e713-37b5056f5dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories configured\n",
      "Data will load from: /content/drive/MyDrive/Project4_Privacy_Alignment/data\n",
      "Models will save to: /content/drive/MyDrive/Project4_Privacy_Alignment/models\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# CELL 3: Configure Paths\n",
    "DRIVE_BASE = Path(\"/content/drive/MyDrive/Project4_Privacy_Alignment\")\n",
    "DRIVE_DATA_DIR = DRIVE_BASE / \"data\"\n",
    "DRIVE_MODELS_DIR = DRIVE_BASE / \"models\"\n",
    "DRIVE_RESULTS_DIR = DRIVE_BASE / \"results\"\n",
    "\n",
    "LOCAL_BASE = Path(\"/content\")\n",
    "LOCAL_DATA_DIR = LOCAL_BASE / \"data\"\n",
    "LOCAL_MODELS_DIR = LOCAL_BASE / \"models\"\n",
    "LOCAL_RESULTS_DIR = LOCAL_BASE / \"results\"\n",
    "CHECKPOINT_DIR = LOCAL_BASE / \"checkpoints\"\n",
    "\n",
    "for dir_path in [LOCAL_DATA_DIR, LOCAL_MODELS_DIR, LOCAL_RESULTS_DIR,\n",
    "                 CHECKPOINT_DIR, DRIVE_MODELS_DIR, DRIVE_RESULTS_DIR]:\n",
    "    dir_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"Directories configured\")\n",
    "print(f\"Data will load from: {DRIVE_DATA_DIR}\")\n",
    "print(f\"Models will save to: {DRIVE_MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Q4O6k65ZOGN",
    "outputId": "5247ad16-d1eb-4528-fe29-a77be68ed267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizers...\n",
      "Tokenizers loaded\n",
      "   Policy: gpt2\n",
      "   Reward: distilbert-base-uncased\n",
      "   MAX_LENGTH: 224\n",
      "   Policy padding side: right\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: Initialize Tokenizers - FIXED\n",
    "print(\"\\nLoading tokenizers...\")\n",
    "\n",
    "# Policy tokenizer (GPT-2) - RIGHT PADDING FOR TRAINING\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(config['policy_model'])\n",
    "policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "policy_tokenizer.padding_side = 'right'  # CRITICAL: Right padding for training\n",
    "\n",
    "# Reward tokenizer (DistilBERT)\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(config['reward_model'])\n",
    "\n",
    "MAX_LENGTH = 224\n",
    "\n",
    "print(f\"Tokenizers loaded\")\n",
    "print(f\"   Policy: {config['policy_model']}\")\n",
    "print(f\"   Reward: {config['reward_model']}\")\n",
    "print(f\"   MAX_LENGTH: {MAX_LENGTH}\")\n",
    "print(f\"   Policy padding side: {policy_tokenizer.padding_side}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "1b4a9cc5551d49d2a67911451f9df294",
      "4195daec1af14703b48796a82e78afc8",
      "6b4af495e6ac4994b242f76ee0b140cd",
      "8847f8750bd04dae89104995046c69e8",
      "fcd657a7c0cb4e0487309281f2f17f5c",
      "97661ee159db4f429499a7a96a5e6f53",
      "4b5ece5629594714b6cce2b5f38b3be1",
      "67ad50f7877b48e2b04b33023dc5daa2",
      "0f0b1d794b6a4934a13c771ff25edf61",
      "bfe7860f1deb4af8a8d66358cb2c675a",
      "61a62ea2981b477e859c9a3615bbb560"
     ]
    },
    "id": "-SdOm-JdZOJm",
    "outputId": "e15d0132-54cf-4da5-bf9d-2110db3530fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing reward model training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4a9cc5551d49d2a67911451f9df294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing reward data:   0%|          | 0/18000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward data prepared: 18000 pairs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# CELL 6: Prepare Reward Model Data\n",
    "print(\"\\nPreparing reward model training data...\")\n",
    "\n",
    "def prepare_reward_data(examples):\n",
    "    \"\"\"Prepare data for reward model (preference pairs)\"\"\"\n",
    "    texts_chosen = []\n",
    "    texts_rejected = []\n",
    "\n",
    "    for prompt, chosen, rejected in zip(\n",
    "        examples['prompt'],\n",
    "        examples['chosen'],\n",
    "        examples['rejected']\n",
    "    ):\n",
    "        text_chosen = f\"{prompt} {chosen}\"\n",
    "        text_rejected = f\"{prompt} {rejected}\"\n",
    "\n",
    "        texts_chosen.append(text_chosen)\n",
    "        texts_rejected.append(text_rejected)\n",
    "\n",
    "    chosen_encodings = reward_tokenizer(\n",
    "        texts_chosen,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "    rejected_encodings = reward_tokenizer(\n",
    "        texts_rejected,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_ids_chosen': chosen_encodings['input_ids'],\n",
    "        'attention_mask_chosen': chosen_encodings['attention_mask'],\n",
    "        'input_ids_rejected': rejected_encodings['input_ids'],\n",
    "        'attention_mask_rejected': rejected_encodings['attention_mask'],\n",
    "    }\n",
    "\n",
    "reward_train = train_dataset.map(\n",
    "    prepare_reward_data,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Preparing reward data\"\n",
    ")\n",
    "\n",
    "print(f\"Reward data prepared: {len(reward_train)} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rWW3a8WTZZb-",
    "outputId": "a449afaa-a315-4e49-c041-d01c66929329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# CELL 7: Helper Functions\n",
    "def save_model_to_drive(model, tokenizer, save_name, metrics, training_time):\n",
    "    \"\"\"Save model to both local and Drive\"\"\"\n",
    "    local_path = LOCAL_MODELS_DIR / save_name\n",
    "    local_path.mkdir(exist_ok=True)\n",
    "\n",
    "    model.save_pretrained(local_path)\n",
    "    tokenizer.save_pretrained(local_path)\n",
    "\n",
    "    results = {\n",
    "        'metrics': metrics,\n",
    "        'training_time': training_time,\n",
    "        'config': config,\n",
    "        'max_length': MAX_LENGTH\n",
    "    }\n",
    "\n",
    "    with open(local_path / 'results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(f\"   Saved to local: {local_path}\")\n",
    "\n",
    "    drive_path = DRIVE_MODELS_DIR / save_name\n",
    "    if drive_path.exists():\n",
    "        shutil.rmtree(drive_path)\n",
    "\n",
    "    shutil.copytree(local_path, drive_path)\n",
    "    print(f\"   Copied to Drive: {drive_path}\")\n",
    "\n",
    "print(\"Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 874
    },
    "id": "etV0PTnZZcRe",
    "outputId": "8f11e398-bd95-432f-a7e8-f58aef0a0c28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 1: Train Reward Model\n",
      "============================================================\n",
      "Loading reward model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting reward model training (3 epochs)...\n",
      "Note: Using fp32 for stability (reward model is small, still fast)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3375' max='3375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3375/3375 11:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.692100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.681700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.684700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.676000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.675900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.673200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.667900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.664800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.672800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.667700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model complete in 11.4 minutes\n",
      "   Saved to local: /content/models/reward_model\n",
      "   Copied to Drive: /content/drive/MyDrive/Project4_Privacy_Alignment/models/reward_model\n",
      "\n",
      "Reward model ready for RLHF training\n"
     ]
    }
   ],
   "source": [
    "# CELL 8: Train Reward Model - FIXED (No fp16)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: Train Reward Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class RewardTrainer(Trainer):\n",
    "    \"\"\"Custom trainer for reward model\"\"\"\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Move tensors to correct device\n",
    "        input_ids_chosen = inputs['input_ids_chosen'].to(model.device)\n",
    "        attention_mask_chosen = inputs['attention_mask_chosen'].to(model.device)\n",
    "        input_ids_rejected = inputs['input_ids_rejected'].to(model.device)\n",
    "        attention_mask_rejected = inputs['attention_mask_rejected'].to(model.device)\n",
    "\n",
    "        rewards_chosen = model(\n",
    "            input_ids=input_ids_chosen,\n",
    "            attention_mask=attention_mask_chosen\n",
    "        ).logits\n",
    "\n",
    "        rewards_rejected = model(\n",
    "            input_ids=input_ids_rejected,\n",
    "            attention_mask=attention_mask_rejected\n",
    "        ).logits\n",
    "\n",
    "        # Maximize margin between chosen and rejected\n",
    "        loss = -torch.nn.functional.logsigmoid(\n",
    "            rewards_chosen - rewards_rejected\n",
    "        ).mean()\n",
    "\n",
    "        if return_outputs:\n",
    "            return loss, {'rewards_chosen': rewards_chosen, 'rewards_rejected': rewards_rejected}\n",
    "        return loss\n",
    "\n",
    "# Custom data collator for reward model\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class RewardDataCollator:\n",
    "    \"\"\"Data collator for reward model that handles chosen/rejected pairs\"\"\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = {\n",
    "            'input_ids_chosen': [],\n",
    "            'attention_mask_chosen': [],\n",
    "            'input_ids_rejected': [],\n",
    "            'attention_mask_rejected': [],\n",
    "        }\n",
    "\n",
    "        for feature in features:\n",
    "            batch['input_ids_chosen'].append(feature['input_ids_chosen'])\n",
    "            batch['attention_mask_chosen'].append(feature['attention_mask_chosen'])\n",
    "            batch['input_ids_rejected'].append(feature['input_ids_rejected'])\n",
    "            batch['attention_mask_rejected'].append(feature['attention_mask_rejected'])\n",
    "\n",
    "        # Convert to tensors\n",
    "        batch = {\n",
    "            k: torch.tensor(v, dtype=torch.long)\n",
    "            for k, v in batch.items()\n",
    "        }\n",
    "\n",
    "        return batch\n",
    "\n",
    "print(\"Loading reward model...\")\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config['reward_model'],\n",
    "    num_labels=1,\n",
    "    # Don't use torch_dtype, let it use default fp32\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "reward_args = TrainingArguments(\n",
    "    output_dir=str(CHECKPOINT_DIR / \"reward_model\"),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=False,  # CRITICAL: Disable fp16 for reward model\n",
    "    logging_steps=200,\n",
    "    save_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create data collator\n",
    "reward_collator = RewardDataCollator()\n",
    "\n",
    "reward_trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=reward_args,\n",
    "    train_dataset=reward_train,\n",
    "    data_collator=reward_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting reward model training (3 epochs)...\")\n",
    "print(\"Note: Using fp32 for stability (reward model is small, still fast)\")\n",
    "start_time = time.time()\n",
    "reward_result = reward_trainer.train()\n",
    "reward_time = time.time() - start_time\n",
    "\n",
    "print(f\"Reward model complete in {reward_time/60:.1f} minutes\")\n",
    "\n",
    "save_model_to_drive(\n",
    "    reward_model,\n",
    "    reward_tokenizer,\n",
    "    \"reward_model\",\n",
    "    reward_result.metrics,\n",
    "    reward_time\n",
    ")\n",
    "\n",
    "print(\"\\nReward model ready for RLHF training\")\n",
    "\n",
    "del reward_trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxFRDhowZk9s",
    "outputId": "57be4e9e-a14b-471e-8189-6cb6bb3d8af2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RLHF training function ready\n"
     ]
    }
   ],
   "source": [
    "# CELL 9: RLHF Training Function - COMPLETE FIX\n",
    "def train_simple_rlhf(model_name, epsilon=None, num_epochs=2):\n",
    "    \"\"\"\n",
    "    Simplified RLHF training (SFT on preferred responses)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    if epsilon:\n",
    "        print(f\"Training DP-RLHF with epsilon={epsilon}\")\n",
    "    else:\n",
    "        print(f\"Training RLHF Baseline\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Load policy model\n",
    "    print(\"Loading policy model...\")\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "    # Load SFT baseline as starting point\n",
    "    sft_path = LOCAL_MODELS_DIR / \"sft_baseline\"\n",
    "    if sft_path.exists():\n",
    "        print(f\"   Loading from SFT baseline: {sft_path}\")\n",
    "        policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            sft_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map='auto'\n",
    "        )\n",
    "    else:\n",
    "        print(f\"   Loading fresh model\")\n",
    "        policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "            config['policy_model'],\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map='auto'\n",
    "        )\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=config.get('lora_r', 8),\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"c_attn\", \"c_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "\n",
    "        policy_model = get_peft_model(policy_model, lora_config)\n",
    "\n",
    "    policy_model.print_trainable_parameters()\n",
    "\n",
    "    # Load reward model (for reference, not used in loss)\n",
    "    print(\"Loading reward model...\")\n",
    "    reward_model_local = AutoModelForSequenceClassification.from_pretrained(\n",
    "        LOCAL_MODELS_DIR / \"reward_model\",\n",
    "        device_map='auto'\n",
    "    )\n",
    "    reward_model_local.eval()\n",
    "\n",
    "    # Prepare training data - SIMPLE VERSION\n",
    "    print(\"Preparing training data...\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        \"\"\"Simple tokenization with proper label handling\"\"\"\n",
    "        texts = []\n",
    "        for prompt, chosen in zip(examples['prompt'], examples['chosen']):\n",
    "            # Format: Human: {prompt}\\n\\nAssistant: {chosen}\n",
    "            text = f\"Human: {prompt}\\n\\nAssistant: {chosen}\"\n",
    "            texts.append(text)\n",
    "\n",
    "        # Tokenize\n",
    "        model_inputs = policy_tokenizer(\n",
    "            texts,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # Set labels (copy input_ids, will handle padding below)\n",
    "        model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    # Apply tokenization\n",
    "    rlhf_train_data = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "        desc=\"Tokenizing data\"\n",
    "    )\n",
    "\n",
    "    # Create custom data collator to handle padding in labels\n",
    "    from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=policy_tokenizer,\n",
    "        mlm=False,  # We're doing causal LM, not masked LM\n",
    "    )\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(CHECKPOINT_DIR / model_name),\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=5e-5,\n",
    "        fp16=(epsilon is None),\n",
    "        logging_steps=200,\n",
    "        save_strategy=\"epoch\",\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Standard Trainer\n",
    "    trainer = Trainer(\n",
    "        model=policy_model,\n",
    "        args=training_args,\n",
    "        train_dataset=rlhf_train_data,\n",
    "        data_collator=data_collator,\n",
    "        processing_class=policy_tokenizer,\n",
    "    )\n",
    "\n",
    "    # Add DP if epsilon specified\n",
    "    if epsilon is not None:\n",
    "        print(f\"Configuring privacy engine with epsilon={epsilon}...\")\n",
    "        policy_model = ModuleValidator.fix(policy_model)\n",
    "\n",
    "        try:\n",
    "            privacy_engine = PrivacyEngine()\n",
    "            policy_model, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n",
    "                module=trainer.model,\n",
    "                optimizer=trainer.optimizer,\n",
    "                data_loader=trainer.get_train_dataloader(),\n",
    "                epochs=num_epochs,\n",
    "                target_epsilon=epsilon,\n",
    "                target_delta=config.get('delta', 1e-5),\n",
    "                max_grad_norm=1.0,\n",
    "            )\n",
    "            print(f\"   Privacy engine configured\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Warning: Privacy engine setup failed: {e}\")\n",
    "            print(f\"   Continuing with gradient clipping only\")\n",
    "\n",
    "    # Train\n",
    "    print(f\"Starting training ({num_epochs} epochs)...\")\n",
    "    start_time = time.time()\n",
    "    result = trainer.train()\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Training complete in {training_time/60:.1f} minutes\")\n",
    "    print(f\"Final training loss: {result.metrics.get('train_loss', 'N/A')}\")\n",
    "\n",
    "    # Save\n",
    "    metrics = result.metrics\n",
    "    if epsilon:\n",
    "        try:\n",
    "            epsilon_spent = privacy_engine.get_epsilon(config.get('delta', 1e-5))\n",
    "            metrics['epsilon_spent'] = epsilon_spent\n",
    "            print(f\"   Final epsilon spent: {epsilon_spent:.2f}\")\n",
    "        except:\n",
    "            metrics['epsilon_target'] = epsilon\n",
    "\n",
    "    save_model_to_drive(policy_model, policy_tokenizer, model_name, metrics, training_time)\n",
    "\n",
    "    # Cleanup\n",
    "    del policy_model, reward_model_local, trainer\n",
    "    try:\n",
    "        del privacy_engine\n",
    "    except:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return metrics, training_time\n",
    "\n",
    "print(\"RLHF training function ready\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c147492864584b7bb55388e578b0d342",
      "8fb40ad4fc5f44fa945578cac961b51b",
      "8f9fd00daa6a4a43adb69bf839ce41c4",
      "f3e7184db2fa42c9a0cce689e84fe90b",
      "84743b56c62b4d85b433512f21d9a480",
      "86a5353fe3c345ae954dd58db1f2d177",
      "f9a6e65eb3d4420c90f30ae3c6682b3b",
      "18335ea5810a4a21957e082c72fde219",
      "edc06f4e8a1d48cb87bb54851af4e846",
      "a936d0c0f30546079e1f9b1c432a0b03",
      "f359605457914f38919c7d6016dd608d"
     ]
    },
    "id": "YdWQfDmJltor",
    "outputId": "475c0056-9bc9-40bb-c24d-eeb73d82b3fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: RLHF Baseline (No Privacy)\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training RLHF Baseline\n",
      "============================================================\n",
      "Loading policy model...\n",
      "   Loading fresh model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n",
      "Loading reward model...\n",
      "Preparing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c147492864584b7bb55388e578b0d342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing data:   0%|          | 0/18000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training (4 epochs)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 30:04, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.976000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.587900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.561900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.530700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.541900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.513400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.521300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.505200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.497500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.502600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.502800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.502300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.482900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.507000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 30.1 minutes\n",
      "Final training loss: 2.5488104010687933\n",
      "   Saved to local: /content/models/rlhf_baseline\n",
      "   Copied to Drive: /content/drive/MyDrive/Project4_Privacy_Alignment/models/rlhf_baseline\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CELL 10: Train RLHF Baseline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: RLHF Baseline (No Privacy)\")\n",
    "print(\"=\"*60)\n",
    "metrics_rlhf, time_rlhf = train_simple_rlhf(\"rlhf_baseline\", epsilon=None, num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IGmSi1XgZnCA",
    "outputId": "e6f8a0b4-c639-48bd-9603-0d75403e81e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: DP-RLHF epsilon=8\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training DP-RLHF with epsilon=8.0\n",
      "============================================================\n",
      "Loading policy model...\n",
      "   Loading fresh model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n",
      "Loading reward model...\n",
      "Preparing training data...\n",
      "Configuring privacy engine with epsilon=8.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Warning: Privacy engine setup failed: 'NoneType' object has no attribute 'param_groups'\n",
      "   Continuing with gradient clipping only\n",
      "Starting training (4 epochs)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 30:03, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.976300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.669100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.588200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.561800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.540900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.512900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.519400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.503300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.500700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.500600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.482300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.494600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.481400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.505700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 30.1 minutes\n",
      "Final training loss: 2.547619137234158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/opacus/accountants/prv.py:151: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mesh_size = eps_error / np.sqrt(\n",
      "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/domain.py:43: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  t_min = np.floor(t_min / dt) * dt\n",
      "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/domain.py:44: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  t_max = np.ceil(t_max / dt) * dt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saved to local: /content/models/dp_rlhf_eps8.0\n",
      "   Copied to Drive: /content/drive/MyDrive/Project4_Privacy_Alignment/models/dp_rlhf_eps8.0\n"
     ]
    }
   ],
   "source": [
    "# CELL 11: Train DP-RLHF epsilon=8\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: DP-RLHF epsilon=8\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics_dp8, time_dp8 = train_simple_rlhf(\"dp_rlhf_eps8.0\", epsilon=8.0, num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mMf66YtcZqGG",
    "outputId": "9845f7ab-bdda-482d-ddd5-fa6987d6a04c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 4: DP-RLHF epsilon=1\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training DP-RLHF with epsilon=1.0\n",
      "============================================================\n",
      "Loading policy model...\n",
      "   Loading fresh model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n",
      "Loading reward model...\n",
      "Preparing training data...\n",
      "Configuring privacy engine with epsilon=1.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Warning: Privacy engine setup failed: 'NoneType' object has no attribute 'param_groups'\n",
      "   Continuing with gradient clipping only\n",
      "Starting training (4 epochs)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 30:00, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.976300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.588300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.561700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.529600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.540800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.534100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.512900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.519400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.503300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.500800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.500700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.482300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.494500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.481300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.505700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ca29b1ba-bc49-4b94-9030-456cd5c4d964)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/config.json\n",
      "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ca29b1ba-bc49-4b94-9030-456cd5c4d964)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 30.0 minutes\n",
      "Final training loss: 2.547623918321398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/opacus/accountants/prv.py:151: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mesh_size = eps_error / np.sqrt(\n",
      "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/domain.py:43: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  t_min = np.floor(t_min / dt) * dt\n",
      "/usr/local/lib/python3.12/dist-packages/opacus/accountants/analysis/prv/domain.py:44: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  t_max = np.ceil(t_max / dt) * dt\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 53a57d57-dfc3-4fb6-9e49-83a6c58073c4)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/config.json\n",
      "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 53a57d57-dfc3-4fb6-9e49-83a6c58073c4)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Saved to local: /content/models/dp_rlhf_eps1.0\n",
      "   Copied to Drive: /content/drive/MyDrive/Project4_Privacy_Alignment/models/dp_rlhf_eps1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CELL 12: Optional - Train DP-RLHF epsilon=1\n",
    "# Uncomment if you have time budget\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: DP-RLHF epsilon=1\")\n",
    "print(\"=\"*60)\n",
    "metrics_dp1, time_dp1 = train_simple_rlhf(\"dp_rlhf_eps1.0\", epsilon=1.0, num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umZjl_IoYzPN",
    "outputId": "db14e4fd-88ae-4a34-dd36-6adf233580d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RLHF TRACK COMPLETE\n",
      "============================================================\n",
      "\n",
      "Models trained: 3\n",
      "   Success: reward_model\n",
      "   Success: rlhf_baseline\n",
      "   Success: dp_rlhf_eps8.0\n",
      "\n",
      "All models saved to Drive: /content/drive/MyDrive/Project4_Privacy_Alignment/models\n",
      "\n",
      "Training time summary:\n",
      "   Reward model (3 epochs): 11.4 min\n",
      "   RLHF baseline (4 epochs): 30.1 min\n",
      "   DP-RLHF eps=8 (4 epochs): 30.1 min\n",
      "   DP-RLHF eps=1 (4 epochs): 30.0 min\n",
      "\n",
      "Configuration used:\n",
      "   Samples: 18000\n",
      "   Epochs: 4 (RLHF), 3 (reward)\n",
      "   MAX_LENGTH: 224\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CELL 13: Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RLHF TRACK COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models_trained = [\n",
    "    \"reward_model\",\n",
    "    \"rlhf_baseline\",\n",
    "    \"dp_rlhf_eps8.0\",\n",
    "]\n",
    "\n",
    "print(f\"\\nModels trained: {len(models_trained)}\")\n",
    "for model_name in models_trained:\n",
    "    drive_path = DRIVE_MODELS_DIR / model_name\n",
    "    if drive_path.exists():\n",
    "        print(f\"   Success: {model_name}\")\n",
    "\n",
    "print(f\"\\nAll models saved to Drive: {DRIVE_MODELS_DIR}\")\n",
    "\n",
    "print(\"\\nTraining time summary:\")\n",
    "total_time = reward_time + time_rlhf + time_dp8\n",
    "print(f\"   Reward model (3 epochs): {reward_time/60:.1f} min\")\n",
    "print(f\"   RLHF baseline (4 epochs): {time_rlhf/60:.1f} min\")\n",
    "print(f\"   DP-RLHF eps=8 (4 epochs): {time_dp8/60:.1f} min\")\n",
    "print(f\"   DP-RLHF eps=1 (4 epochs): {time_dp1/60:.1f} min\")\n",
    "\n",
    "\n",
    "print(\"\\nConfiguration used:\")\n",
    "print(f\"   Samples: {len(train_dataset)}\")\n",
    "print(f\"   Epochs: 4 (RLHF), 3 (reward)\")\n",
    "print(f\"   MAX_LENGTH: {MAX_LENGTH}\")\n",
    "\n",
    "\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
