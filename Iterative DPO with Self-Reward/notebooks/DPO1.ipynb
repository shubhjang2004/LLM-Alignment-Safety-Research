{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19283,
     "status": "ok",
     "timestamp": 1761249066709,
     "user": {
      "displayName": "shivanimeena",
      "userId": "14777463049176070184"
     },
     "user_tz": -330
    },
    "id": "rBml0tUWSNgp",
    "outputId": "54c51892-2a07-4a13-b4ef-663505a6e397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15312,
     "status": "ok",
     "timestamp": 1761249087180,
     "user": {
      "displayName": "shivanimeena",
      "userId": "14777463049176070184"
     },
     "user_tz": -330
    },
    "id": "9-umtBndSQ5W",
    "outputId": "d648f3bb-5876-4e7b-e3c7-90bc2e541a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/423.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m419.8/423.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install trl --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57565,
     "status": "ok",
     "timestamp": 1761249148616,
     "user": {
      "displayName": "shivanimeena",
      "userId": "14777463049176070184"
     },
     "user_tz": -330
    },
    "id": "a3iayazxdbqB",
    "outputId": "2f47f81c-fc8d-435e-ed18-e464f514a2d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# DPO Training Round 1 - Fixed Version for Colab\n",
    "# Run this AFTER generating synthetic preferences\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import json\n",
    "import os\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    sft_model_dir = \"/content/drive/MyDrive/outputs/sft_model\"\n",
    "    dpo_round1_dir = \"/content/drive/MyDrive/outputs/dpo_round1\"\n",
    "    synthetic_data_path = \"/content/drive/MyDrive/outputs/synthetic_preferences.json\"\n",
    "    dataset_name = \"Anthropic/hh-rlhf\"\n",
    "    max_length = 512\n",
    "    max_prompt_length = 256\n",
    "    dpo_epochs = 3\n",
    "    batch_size = 1  # Reduced for memory\n",
    "    gradient_accumulation_steps = 16  # Increased to compensate\n",
    "    learning_rate = 5e-5\n",
    "    beta = 0.1\n",
    "    lora_r = 16\n",
    "    lora_alpha = 32\n",
    "    lora_dropout = 0.05\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.dpo_round1_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1126,
     "status": "ok",
     "timestamp": 1761249183491,
     "user": {
      "displayName": "shivanimeena",
      "userId": "14777463049176070184"
     },
     "user_tz": -330
    },
    "id": "Osy3YVyZdbsv",
    "outputId": "f0b4a9e6-a9c1-4942-964d-c8edf50bedac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading preference data...\n",
      "Total preferences: 3955 (Original: 2999, Synthetic: 956)\n",
      "Train samples: 3559, Eval samples: 396\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.sft_model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # DPO requires right padding\n",
    "\n",
    "# Load data - combine original + synthetic\n",
    "print(\"Loading preference data...\")\n",
    "\n",
    "# Original preferences\n",
    "original_dataset = load_dataset(config.dataset_name, split=\"train\")\n",
    "original_dataset = original_dataset.select(range(min(3000, len(original_dataset))))\n",
    "\n",
    "original_prefs = []\n",
    "for ex in original_dataset:\n",
    "    try:\n",
    "        prompt = ex['chosen'].split('Assistant:')[0].replace('Human:', '').strip()\n",
    "        chosen = ex['chosen'].split('Assistant:')[-1].strip()\n",
    "        rejected = ex['rejected'].split('Assistant:')[-1].strip()\n",
    "        if prompt and chosen and rejected:\n",
    "            original_prefs.append({\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected})\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Synthetic preferences\n",
    "with open(config.synthetic_data_path, 'r') as f:\n",
    "    synthetic_prefs = json.load(f)\n",
    "\n",
    "# Remove score fields that DPO doesn't expect\n",
    "synthetic_prefs_cleaned = []\n",
    "for pref in synthetic_prefs:\n",
    "    synthetic_prefs_cleaned.append({\n",
    "        \"prompt\": pref[\"prompt\"],\n",
    "        \"chosen\": pref[\"chosen\"],\n",
    "        \"rejected\": pref[\"rejected\"]\n",
    "    })\n",
    "\n",
    "# Combine\n",
    "all_prefs = original_prefs + synthetic_prefs_cleaned\n",
    "print(f\"Total preferences: {len(all_prefs)} (Original: {len(original_prefs)}, Synthetic: {len(synthetic_prefs_cleaned)})\")\n",
    "\n",
    "# Create dataset\n",
    "dpo_dataset = Dataset.from_list(all_prefs)\n",
    "\n",
    "# Split into train/eval\n",
    "train_test_split = dpo_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "print(f\"Train samples: {len(train_dataset)}, Eval samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "87512cf7a9664870b4622931983ae85b",
      "a049b3222ef2440b93da6707b4e2a59b",
      "52623abfbb314f11afd11bd2f69359b7",
      "f0a1c82a6425480caf9d3ec3051b2ba5",
      "e7798bdc85704cfcb99b551693a95929",
      "2945f26fcc6248e1ad85bb56e83128db",
      "412758e9d3c04faea431b80f0d4232c3",
      "f2d7463cb61d423dbbf6103a75268885",
      "017003646a054c509c792a545ab1edca",
      "4472a18eaf3349bb87c7640e6ab5fb46",
      "be33d6686798478980c3c29c8b344aa0",
      "4cae663a750e4b06b9a2174c9994a0f7",
      "d0e3d20bab874dd4bd212c8e2aeba107",
      "c9349ed7d307402aa920e19d114c399c",
      "d2d9a3e52cff4488a7e688a5e2a86f35",
      "2e8d87b0b8ab49ec9e04039d246ff3ee",
      "61148980e72b468b8e195c2d874d0a2e",
      "d76df425f4cf4095adac9f618b2b6b2f",
      "97133527bfb34e18bf8f8114d3150b14",
      "23ac2bd165fa4ff8b9cfa7fa8654c35b",
      "e1a1ba9e42bf4c4f973cb99d910ed82b",
      "7e91cebffc584ea68aec533338d245a2",
      "9f3b7e6c887949c5a2e67fc4af93c15c",
      "7c3f15faf828432bb3ba33e028bfbcf0",
      "e7e1b5002c704c119f2a1b4282c51653",
      "faf58a08e02548e396334a2384d5ffc6",
      "88a50fc27486412d8fabf6af787e7c7a",
      "e8270f18cdc54d4498f846e2e28eb84b",
      "2c1618d2f5ee4ab9a8a291f5d24f22cd",
      "acd97c2545234c1ab0cb010c2ebb280a",
      "9aadb78322264ffbab6250677f8e6a70",
      "e59ed211d50b4dcca30541035deaa0fd",
      "a64b327741404e45818e9896cb157832",
      "b75b6a7deed34ac29da40e43446d97eb",
      "fa6a327873f04b4ebd99ca96e8126f83",
      "bfb011f8e8ef4103bd6855f126c09c13",
      "4bffe4cd59324460829f2731799d1bfe",
      "5b9c10e101bb4b0abd51747e3b39e3af",
      "e93ad088b81b4bbb81cf411f29ce8f5c",
      "c6e3519e6e6d448590e4bab2c4ce3bf9",
      "6347386ff04c45998b873199327877eb",
      "fc696cebe7a54ba19f7520fc407ffb30",
      "0aed5934b82b4c1283bcf0c90f43859c",
      "0fac0f7d8ea94465bacf6554827064b8",
      "9246555f8177418a88308415e4ef6580",
      "5488404e2ed64226a86f4dcb98e2dc98",
      "e97a2f28e86749aba9152b857a3da7c5",
      "ec6bfdce73974d1ab4b4a63da344206f",
      "0cbcda0c18d74187a011d990c4eae6bd",
      "68f3d232878c48ef9064098a105a84f0",
      "454e5590aa4d450480e36828beb99762",
      "7d0fb1ce0a2c40079ff7a2448a3af83d",
      "1fc74c632da440579f358eaa7adf83e6",
      "426ec94ea9264e1ebe8bb1cf40acf289",
      "61fa5d68acfa451c945fecf337a2b0f5",
      "3459e3cc35884053912fa72b314eb710",
      "4f783a05e0b64d47862dfa970abfc861",
      "6222175c4d424d0bb03cc62157a198ba",
      "47884a59587e4f97ba5eec07525255dd",
      "653ab3d5c41a49fc90a5ed500f015a43",
      "e4c48ac5ebd1421fa7f81d3669ad0634",
      "fb93588d7d204afaabc843e0df3e489b",
      "7f55fb5d14954caca92a04aed117be0b",
      "f0ec59dbb62b4c4fbbdb9810a409faee",
      "8f8f06b703034b468c1e8c02891bf3a0",
      "dacd2acc2bdf4333adccad3e59dc97a4"
     ]
    },
    "executionInfo": {
     "elapsed": 4029778,
     "status": "ok",
     "timestamp": 1761253331618,
     "user": {
      "displayName": "shivanimeena",
      "userId": "14777463049176070184"
     },
     "user_tz": -330
    },
    "id": "8DVKdX0RdaH4",
    "outputId": "bdddb074-3634-496d-c58f-116619008863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SFT model...\n",
      "Trainable parameters:\n",
      "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n",
      "Creating DPO trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87512cf7a9664870b4622931983ae85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/3559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cae663a750e4b06b9a2174c9994a0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/3559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3b7e6c887949c5a2e67fc4af93c15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3559 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75b6a7deed34ac29da40e43446d97eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset:   0%|          | 0/396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9246555f8177418a88308415e4ef6580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3459e3cc35884053912fa72b314eb710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting DPO Round 1 training...\n",
      "Effective batch size: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='669' max='669' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [669/669 1:06:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.627100</td>\n",
       "      <td>0.634764</td>\n",
       "      <td>-0.192962</td>\n",
       "      <td>-0.379820</td>\n",
       "      <td>0.651515</td>\n",
       "      <td>0.186858</td>\n",
       "      <td>-100.655296</td>\n",
       "      <td>-124.265381</td>\n",
       "      <td>-3.606921</td>\n",
       "      <td>-3.621131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.576500</td>\n",
       "      <td>0.611591</td>\n",
       "      <td>-0.536880</td>\n",
       "      <td>-0.985643</td>\n",
       "      <td>0.641414</td>\n",
       "      <td>0.448762</td>\n",
       "      <td>-104.094482</td>\n",
       "      <td>-130.323593</td>\n",
       "      <td>-3.595548</td>\n",
       "      <td>-3.612767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.474300</td>\n",
       "      <td>0.628842</td>\n",
       "      <td>-1.123024</td>\n",
       "      <td>-1.792914</td>\n",
       "      <td>0.669192</td>\n",
       "      <td>0.669890</td>\n",
       "      <td>-109.955894</td>\n",
       "      <td>-138.396317</td>\n",
       "      <td>-3.595815</td>\n",
       "      <td>-3.618399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.489400</td>\n",
       "      <td>0.630253</td>\n",
       "      <td>-1.119994</td>\n",
       "      <td>-1.741365</td>\n",
       "      <td>0.661616</td>\n",
       "      <td>0.621371</td>\n",
       "      <td>-109.925613</td>\n",
       "      <td>-137.880814</td>\n",
       "      <td>-3.620692</td>\n",
       "      <td>-3.644522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.648646</td>\n",
       "      <td>-1.287266</td>\n",
       "      <td>-1.979265</td>\n",
       "      <td>0.651515</td>\n",
       "      <td>0.691999</td>\n",
       "      <td>-111.598328</td>\n",
       "      <td>-140.259827</td>\n",
       "      <td>-3.616410</td>\n",
       "      <td>-3.642539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.357500</td>\n",
       "      <td>0.655378</td>\n",
       "      <td>-1.373751</td>\n",
       "      <td>-2.096601</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.722850</td>\n",
       "      <td>-112.463165</td>\n",
       "      <td>-141.433182</td>\n",
       "      <td>-3.614664</td>\n",
       "      <td>-3.643008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving DPO Round 1 LoRA adapters...\n",
      "✓ LoRA adapters saved to /content/drive/MyDrive/outputs/dpo_round1\n",
      "\n",
      "==================================================\n",
      "Testing generation with LoRA...\n",
      "Human: How can I improve my productivity?\n",
      "\n",
      "Assistant: There are a few things you can try to improve your productivity:\n",
      "\n",
      "1. Set a specific goal: Determine what you want to achieve and create a timeline for completing it. This will help you stay focused and motivated.\n",
      "\n",
      "2. Eliminate distractions: Turn off notifications on your phone, close unnecessary tabs on your computer, and set reminders to check email or social media periodically.\n",
      "\n",
      "3. Prioritize tasks: Make\n",
      "==================================================\n",
      "\n",
      "✓ DPO Round 1 Training Complete!\n",
      "\n",
      "To load this model later:\n",
      "from peft import PeftModel\n",
      "base_model = AutoModelForCausalLM.from_pretrained('/content/drive/MyDrive/outputs/sft_model')\n",
      "model = PeftModel.from_pretrained(base_model, '/content/drive/MyDrive/outputs/dpo_round1')\n"
     ]
    }
   ],
   "source": [
    "# Load model with fp16 (not bf16 - Colab T4 doesn't support bf16)\n",
    "print(\"Loading SFT model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.sft_model_dir,\n",
    "    torch_dtype=torch.float16,  # Use fp16 for Colab T4 compatibility\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing before applying LoRA\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Apply LoRA to main model\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"Trainable parameters:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# DPO Training arguments\n",
    "training_args = DPOConfig(\n",
    "    output_dir=config.dpo_round1_dir,\n",
    "    num_train_epochs=config.dpo_epochs,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=config.batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Use fp16 instead of bf16 for Colab\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    beta=config.beta,\n",
    "    max_length=config.max_length,\n",
    "    max_prompt_length=config.max_prompt_length,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    loss_type=\"sigmoid\",\n",
    "    # Memory optimization\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "\n",
    "# Create DPO trainer\n",
    "print(\"Creating DPO trainer...\")\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,  # Let DPOTrainer create it from the base model\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,  # ← CHANGED: Use processing_class instead of tokenizer\n",
    "    # peft_config is already applied to model, so remove it from here\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nStarting DPO Round 1 training...\")\n",
    "print(f\"Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "dpo_trainer.train()\n",
    "\n",
    "# Save LoRA adapters\n",
    "print(\"\\nSaving DPO Round 1 LoRA adapters...\")\n",
    "model.save_pretrained(config.dpo_round1_dir)\n",
    "tokenizer.save_pretrained(config.dpo_round1_dir)\n",
    "print(f\"✓ LoRA adapters saved to {config.dpo_round1_dir}\")\n",
    "\n",
    "# Test generation with LoRA\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing generation with LoRA...\")\n",
    "model.eval()\n",
    "\n",
    "test_prompt = \"Human: How can I improve my productivity?\\n\\nAssistant:\"\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Optionally save merged model (commented out to save time/space)\n",
    "# Uncomment if you want a standalone model without needing to load LoRA adapters\n",
    "\"\"\"\n",
    "print(\"\\nMerging and saving full model...\")\n",
    "model = model.merge_and_unload()\n",
    "merged_dir = config.dpo_round1_dir + \"_merged\"\n",
    "os.makedirs(merged_dir, exist_ok=True)\n",
    "model.save_pretrained(merged_dir)\n",
    "tokenizer.save_pretrained(merged_dir)\n",
    "print(f\"✓ Merged model saved to {merged_dir}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n✓ DPO Round 1 Training Complete!\")\n",
    "print(f\"\\nTo load this model later:\")\n",
    "print(f\"from peft import PeftModel\")\n",
    "print(f\"base_model = AutoModelForCausalLM.from_pretrained('{config.sft_model_dir}')\")\n",
    "print(f\"model = PeftModel.from_pretrained(base_model, '{config.dpo_round1_dir}')\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMt+n+wq07m7ehuOTLltftw",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
