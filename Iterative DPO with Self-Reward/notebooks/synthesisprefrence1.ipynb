{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOuZqNu5vRNFDIVtwswdTYN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eu_J8FpDW3Bg","executionInfo":{"status":"ok","timestamp":1761244786581,"user_tz":-330,"elapsed":20590,"user":{"displayName":"shivanimeena","userId":"14777463049176070184"}},"outputId":"b806eca6-960e-45a5-a3b0-8b83d4f40123"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AutPS--lWuhy","executionInfo":{"status":"ok","timestamp":1761248972812,"user_tz":-330,"elapsed":3863621,"user":{"displayName":"shivanimeena","userId":"14777463049176070184"}},"outputId":"5cfcf132-af72-43da-faf0-18aa596db680"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Loading tokenizer...\n","Loading SFT model...\n","Loading reward model...\n","Model dtype check:\n","Base model dtype: torch.float16\n","Reward head dtype: torch.float16\n","\n","Loading prompts...\n","Number of prompts: 995\n"]},{"output_type":"stream","name":"stderr","text":["Generating responses: 100%|██████████| 995/995 [1:04:19<00:00,  3.88s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Generated 956 synthetic preference pairs\n","✓ Synthetic preferences saved to /content/drive/MyDrive/outputs/synthetic_preferences.json\n","\n","✓ Synthetic Data Generation Complete!\n","\n","=== Statistics ===\n","Average chosen score: 2.3228\n","Average rejected score: 0.7101\n","Average score difference: 1.6127\n","Min score difference: 0.1045\n","Max score difference: 9.9570\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Generate Synthetic Preferences - Fixed Version\n","# Run this AFTER reward model training\n","\n","import torch\n","import numpy as np\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from datasets import load_dataset, Dataset\n","from tqdm import tqdm\n","import json\n","import os\n","import torch.nn as nn\n","\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Configuration\n","class Config:\n","    sft_model_dir = \"/content/drive/MyDrive/outputs/sft_model\"\n","    reward_model_dir = \"/content/drive/MyDrive/outputs/reward_model\"\n","    synthetic_data_path = \"/content/drive/MyDrive/outputs/synthetic_preferences.json\"\n","    dataset_name = \"Anthropic/hh-rlhf\"\n","    num_gen_samples = 1000\n","    max_length = 256\n","    num_responses_per_prompt = 2\n","    temperature = 0.9\n","    top_p = 0.95\n","\n","config = Config()\n","\n","# Load models and tokenizer\n","print(\"Loading tokenizer...\")\n","tokenizer = AutoTokenizer.from_pretrained(config.sft_model_dir)\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","print(\"Loading SFT model...\")\n","sft_model = AutoModelForCausalLM.from_pretrained(\n","    config.sft_model_dir,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","sft_model.eval()\n","\n","# Load reward model\n","class RewardModel(nn.Module):\n","    def __init__(self, base_model):\n","        super().__init__()\n","        self.base_model = base_model\n","        hidden_size = base_model.config.hidden_size\n","        self.reward_head = nn.Sequential(\n","            nn.Linear(hidden_size, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(512, 1)\n","        )\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.base_model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            output_hidden_states=True\n","        )\n","        hidden_states = outputs.hidden_states[-1]\n","        sequence_lengths = attention_mask.sum(dim=1) - 1\n","        batch_size = hidden_states.shape[0]\n","        last_hidden = hidden_states[torch.arange(batch_size, device=hidden_states.device), sequence_lengths]\n","        reward = self.reward_head(last_hidden)\n","        return reward\n","\n","print(\"Loading reward model...\")\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    config.reward_model_dir,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","\n","reward_model = RewardModel(base_model)\n","checkpoint = torch.load(os.path.join(config.reward_model_dir, \"reward_model.pt\"))\n","reward_model.reward_head.load_state_dict(checkpoint['reward_head_state_dict'])\n","\n","# FIX: Convert reward head to float16 to match base model\n","reward_model.reward_head = reward_model.reward_head.to(torch.float16)\n","reward_model = reward_model.to(device)\n","reward_model.eval()\n","\n","print(\"Model dtype check:\")\n","print(f\"Base model dtype: {next(base_model.parameters()).dtype}\")\n","print(f\"Reward head dtype: {next(reward_model.reward_head.parameters()).dtype}\")\n","\n","# Load prompts\n","print(\"\\nLoading prompts...\")\n","dataset = load_dataset(config.dataset_name, split=\"train\")\n","dataset = dataset.select(range(min(config.num_gen_samples, len(dataset))))\n","\n","prompts = []\n","for ex in dataset:\n","    try:\n","        prompt = ex['chosen'].split('Assistant:')[0].replace('Human:', '').strip()\n","        if prompt and len(prompt) > 10:\n","            prompts.append(prompt)\n","    except:\n","        continue\n","\n","prompts = prompts[:config.num_gen_samples]\n","print(f\"Number of prompts: {len(prompts)}\")\n","\n","# Generate responses and score\n","synthetic_data = []\n","\n","for prompt in tqdm(prompts, desc=\"Generating responses\"):\n","    responses = []\n","\n","    # Generate multiple responses\n","    for _ in range(config.num_responses_per_prompt):\n","        input_text = f\"Human: {prompt}\\n\\nAssistant:\"\n","        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=config.max_length).to(device)\n","\n","        with torch.no_grad():\n","            outputs = sft_model.generate(\n","                **inputs,\n","                max_new_tokens=150,\n","                temperature=config.temperature,\n","                top_p=config.top_p,\n","                do_sample=True,\n","                pad_token_id=tokenizer.pad_token_id\n","            )\n","\n","        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        response = full_response.split(\"Assistant:\")[-1].strip()\n","        responses.append(response)\n","\n","    # Score responses with reward model\n","    scores = []\n","    for response in responses:\n","        full_text = f\"Human: {prompt}\\n\\nAssistant: {response}\"\n","        inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=config.max_length, padding=\"max_length\").to(device)\n","\n","        with torch.no_grad():\n","            score = reward_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n","\n","        scores.append(score.item())\n","\n","    # Select chosen and rejected\n","    max_idx = np.argmax(scores)\n","    min_idx = np.argmin(scores)\n","\n","    if max_idx != min_idx and abs(scores[max_idx] - scores[min_idx]) > 0.1:\n","        synthetic_data.append({\n","            \"prompt\": prompt,\n","            \"chosen\": responses[max_idx],\n","            \"rejected\": responses[min_idx],\n","            \"chosen_score\": float(scores[max_idx]),\n","            \"rejected_score\": float(scores[min_idx])\n","        })\n","\n","print(f\"\\nGenerated {len(synthetic_data)} synthetic preference pairs\")\n","\n","# Save synthetic data\n","with open(config.synthetic_data_path, 'w') as f:\n","    json.dump(synthetic_data, f, indent=2)\n","\n","print(f\"✓ Synthetic preferences saved to {config.synthetic_data_path}\")\n","print(\"\\n✓ Synthetic Data Generation Complete!\")\n","\n","# Print statistics\n","if synthetic_data:\n","    chosen_scores = [d['chosen_score'] for d in synthetic_data]\n","    rejected_scores = [d['rejected_score'] for d in synthetic_data]\n","    score_diffs = [d['chosen_score'] - d['rejected_score'] for d in synthetic_data]\n","\n","    print(\"\\n=== Statistics ===\")\n","    print(f\"Average chosen score: {np.mean(chosen_scores):.4f}\")\n","    print(f\"Average rejected score: {np.mean(rejected_scores):.4f}\")\n","    print(f\"Average score difference: {np.mean(score_diffs):.4f}\")\n","    print(f\"Min score difference: {np.min(score_diffs):.4f}\")\n","    print(f\"Max score difference: {np.max(score_diffs):.4f}\")"]}]}