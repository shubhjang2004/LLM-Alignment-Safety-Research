{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuOQ6OtLqMxa",
    "outputId": "a5dbb7d4-d430-4ca9-8b21-336e72d45c09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "WDEFrucSqHAc",
    "outputId": "3aab46ca-b9d0-4574-9999-ad06723c4977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Generate Synthetic Preferences Round 2 - Fixed for Colab\n",
    "# Run this AFTER DPO Round 1 completes\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(43)\n",
    "np.random.seed(43)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    sft_model_dir = \"/content/drive/MyDrive/outputs/sft_model\"  # Base model for LoRA\n",
    "    dpo_round1_dir = \"/content/drive/MyDrive/outputs/dpo_round1\"  # LoRA adapters\n",
    "    reward_model_dir = \"/content/drive/MyDrive/outputs/reward_model\"\n",
    "    synthetic_data_path2 = \"/content/drive/MyDrive/outputs/synthetic_preferences_round2.json\"\n",
    "    dataset_name = \"Anthropic/hh-rlhf\"\n",
    "    num_gen_samples = 500  # Reduced from 1000 for faster generation (~25-30 min)\n",
    "    max_length = 256\n",
    "    num_responses_per_prompt = 2\n",
    "    temperature = 0.85  # Slightly lower than Round 1 for more refined outputs\n",
    "    top_p = 0.95\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e7265a33815d494faddb9b31a0ad3143",
      "fed30425e57a48b6a81a78c0bc63f301",
      "ebb27a25c01646cf9271a4e23560fec0",
      "602027ee1b1a424383dc283f81751cb8",
      "1e36a28e163c4384b60ad773b8b1a3bd",
      "2bc95e9b99ae4bd99f8c6a315e79f67e",
      "ba8c6cac3fff4c37b729903943b8c9b9",
      "6b02c4749342439391788592ffcd2baa",
      "3d07f9b96e4a4fad8b15f58a1fe77c5f",
      "d935fb73c0d94cd0b9d6bf27969ac04c",
      "7c0e23f7969a498c98aaa25649eff530",
      "5f6baed67f9c42b1957ca19ca7369c45",
      "351b68d9b6c6417e954fc49c7fd61a70",
      "15a9d39c66864631bee66d7dd2b57af9",
      "dceec8e3823e4d6f9f3ee546b011c55d",
      "0695c7148161427c8b7b7c34cf591d33",
      "df196f6d638b4a0a8b714e182f8b4b58",
      "4bb151972023462dbdf92dd80ecbc5ee",
      "d01c5b88a16d40e9abddaea6ef30b53b",
      "ff97b2cbd7134416af80866555c849ee",
      "bc98428974af4593a18e68899f54b5e8",
      "e51b9093da2b4d778f859cafab2aba44",
      "32ec64d283964ae3b3ede2960244a750",
      "cb105cc6ac4a4434ac9de7efa1f331a0",
      "8c954fb6bf324ffe9eed31649a86f945",
      "b2213c26f90c4494a5ee97c6f155b05d",
      "a30bc0e07af24d3087a36f7ba7d0021f",
      "42ae1996bf744229b4b13a8a7e04ec42",
      "872ac10162c349bd932be8363ee3f215",
      "5b07cec811984028a3f6add1c55fe767",
      "164eef0f1d9c497b98212378d8f7974d",
      "1a7dd33a2415438e8fc4b4c101b64134",
      "97936676080543adadc245d4bff78bec",
      "4277f769cbfd447d92df7d6aec9197d3",
      "afef04001124453b944cf70e8ca55440",
      "c50a5db18b134ee481752f7da7f6c274",
      "4e13f8c7a4f741c683078d5676455b17",
      "4503cbb6c5e84bea8dcb083286f5471a",
      "3646498d5f9c47a1820cea0f8680e5a2",
      "579ed4e6844d42baafe18f63fb430c8c",
      "2d13a41cc5774da78d5d633fbc2a03a5",
      "329226df90cd453599ec03e6006f7461",
      "13904644f68f495a9a5952112edc98db",
      "8c96e0cbce254eee945d49c585745ec1",
      "ae84c5ed133449dea430d6c37f6130ae",
      "0c8ba63d5e0b42c3a8eacf876b6410ff",
      "4a30441093ba441dba06405c465a3eee",
      "28d199fd2d2f4ee9bd4802226964a4c6",
      "47118f6a51614ef0b54876339362a282",
      "6c50e82f68a34b2c80ed599022018720",
      "d2ef540edcfc47f0b5d88844e92758b7",
      "c5f826151eaf4e809a86b2f2b2ba3024",
      "26e26fc9efa64cdcb337e7a7d9a2c55f",
      "f1af3b43b751477598810294a2aeb553",
      "a27e22be1a1542e4b61341c9fb065ed3",
      "35b7f5d090524e479297617c91f2fa3f",
      "2cfacc8125544787916412035fafdbb9",
      "c2ca815071044650b5726c9e2681da12",
      "470922ce907f4be1bd456d209d5a1938",
      "9451c6d1082a4b3bbc535aaa2d535700",
      "49ead1dcfb1346568d3efcf30c86a632",
      "6507344e15e848f3a9b9c32f36622820",
      "43744cf7d0db47a9acc5ccc221438613",
      "43bdbcb69bf94094b13e1ab3eb818560",
      "f82314852c8046609a81dc1352635d4d",
      "607a3779effa4f329b5970287c952bb0",
      "e002b67c71bd48f9b866eafeabec1b9f",
      "7c2eb2aef1eb443986bd68c7f06692ba",
      "03cf44056cf3462792130caf51406fa6",
      "c867bbe4e7c0422f9512f41c48ae03da",
      "d4e6f5f8fd6a443fa5184e8a4c8ed9c8",
      "00efd43989e24c7faba7422d4d3f4213",
      "8622ed7b2c6f4166a5dfce915ef92d54",
      "4dc6af58e1db4756bd999feea7fe8013",
      "7a9e3dbd19d645f28349b06100fa2c36",
      "3b227d62f5004974a677e439a53ecf50",
      "b13bbff0fb2248cb8c410fdd76f7b6a8",
      "02a112478d6d41f2956bd09ce142c719",
      "1bace3c315e44497b4f1326fc9ae477a",
      "90b6de9618e14fc583fb7c3d88b93636",
      "f51ad4f82e8c46bbac7523f0629e02f9",
      "46ee792b6f2c43ab8a9662844b7fda8e",
      "0e55221764484839ae42e353cea3f0c8",
      "3887bb41331e4cd8b60e24721109d3db",
      "7c5de777984c4c5593a703b1fc6d6e62",
      "b3b5c704b3a744eab268affc6d9d19f1",
      "00d9e2de4c94442086625fd0014c2012",
      "d2cb337c08634876984ec45e9f78092f",
      "f98bdb41e5c944cead79f638ed4813c8",
      "83de0a70be5d4e1d95b6c0b9f18b1a93",
      "e1e747b0c4b44d7daf9e106fb916d6c5",
      "a9274f20cc8a4dcfb22e9507d978db98",
      "9f6042a5505e4a7998840374ead48bce",
      "1e69525695474cb68fb8966502d4fb81",
      "b0bd0b339f8e40298391d1a25a4c7677",
      "b3208da35c2542c89b644000c274fbc4",
      "3210fc593c4c42eea71016ceb9095bc5",
      "300553b375d54d30bfe64c2e74ba6327",
      "39234d5fc4e442728787faf9d41685fc",
      "e02d636bdd304872924c2f931f5c0458",
      "0c8677ba06f94b48ae87ed121463a118",
      "b0af4ebb235742e4a43468c0e0be703f",
      "05e15d085a2d4f96a224a0920752b587",
      "c6d39d5e62f8441daef6554e55a5ecba",
      "64ed1236d46a4b4d910c91684a2f3319",
      "503b086e2be74d219da9e1bc3766afbe",
      "38963aeb196f439e8d7dd6187b321e11",
      "8780312483c84bb6ab3d5af240ba885e",
      "b63be69178274cbe86b72e91d3fd51e3",
      "83804fe6bd8f4af79fc8d8b42475bea5",
      "29bb7826804848e4a2dde2d2212578f3",
      "bffa1ea3ab0e461a8c6df156fdffad8f",
      "719a9b289c74448cb7290103312d718a",
      "d15789fdb8a44a5e903239a78b551405",
      "abc8f333956f4bb7a021bd9ac9e8a70f",
      "e27e3d1781d94c5389d982e073181bed",
      "a6b06f5debc047c6a2e856f6a6606c64",
      "66cb4ffba5004f1c9bbd0e01b0894f5d",
      "e6a519fce09d4c9c9249c4998ed07f2a",
      "f73fc29f052c45ddbdef6280e1b6e7b2",
      "d93af2ecdc87490c8136de74c82ca01f",
      "dd7eb699cf6247918cb82dd7892cf7aa",
      "5a5110f61691452995177f1937a404fd",
      "4c64825cbfbe41fa9d656021ebb0d870",
      "91333387676740459dae9406ad26f627",
      "15758a7181344f94a959ad9ba6145e74",
      "e0a1dfa3487548599ea1ad8055479f0f",
      "4f7da76c5d1d40ce8fdf43a802d3fb70",
      "8e077b5881634460bc4ee659d4766c72",
      "d9975552cd3645468a870196fb28a7bc",
      "999a305ac3ef469fade08111ed664e36",
      "e7db8b8419ac479085fd6082b30b8cc8",
      "0b9a27235837499996b54b05546cccd2",
      "0309ca7778c84b21b6f173801a69c95a",
      "5b1925125cac4e6296a6916e1f42fa37",
      "839720633af14ac787fd413443414d2e",
      "3f94483c5d204dda83678553fa89c3aa",
      "8d901e9c09ff4f4bae15522fcc6baec9",
      "b6d5f2b7ce9d4f5789ffbf1d2b809e4e",
      "97577ed3eaff49fbaa16ac975ced01cb",
      "86debc0838f446db9b43d348b501c535",
      "0974179bffc04e5c99719758e21a61ab",
      "3eeca2df70784ce0a549dee47941583e",
      "03b976fd13f24500b53236e976fd3fe8",
      "375633997abe47fb82a593c956281df5",
      "acadcd00ccf24045a503630198b124c1",
      "ed395582dcba4dc5bec223fb7485784a",
      "93c27480d92e472db6542a1ba7859d1a",
      "fc682f8e1f564cf09b1815afa315f21a",
      "1553b953a7cc4432b5c2a47593dff3e6",
      "09f07457c3864f4da602b313206b4a0f",
      "e088aca11cae45a0b68bc0a285230d8a",
      "29a896761c8e4727adfae4fc2c206adb",
      "acf2edf706f34198bcc2ddf47d2a57f1"
     ]
    },
    "id": "uNPpspzup5be",
    "outputId": "6781074e-71fc-47da-9dad-fc799402700e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length is 256\n",
      "Loading tokenizer...\n",
      "Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7265a33815d494faddb9b31a0ad3143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6baed67f9c42b1957ca19ca7369c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ec64d283964ae3b3ede2960244a750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DPO Round 1 LoRA adapters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reward model...\n",
      "Model dtype check:\n",
      "DPO model dtype: torch.float16\n",
      "Reward head dtype: torch.float16\n",
      "\n",
      "Loading prompts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4277f769cbfd447d92df7d6aec9197d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae84c5ed133449dea430d6c37f6130ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "harmless-base/train.jsonl.gz:   0%|          | 0.00/13.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b7f5d090524e479297617c91f2fa3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-base/train.jsonl.gz:   0%|          | 0.00/16.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e002b67c71bd48f9b866eafeabec1b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-online/train.jsonl.gz:   0%|          | 0.00/20.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a112478d6d41f2956bd09ce142c719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-rejection-sampled/train.jsonl.gz:   0%|          | 0.00/25.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98bdb41e5c944cead79f638ed4813c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "harmless-base/test.jsonl.gz:   0%|          | 0.00/743k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02d636bdd304872924c2f931f5c0458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-base/test.jsonl.gz:   0%|          | 0.00/875k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bb7826804848e4a2dde2d2212578f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-online/test.jsonl.gz:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7eb699cf6247918cb82dd7892cf7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-rejection-sampled/test.jsonl.gz:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9a27235837499996b54b05546cccd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/160800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b976fd13f24500b53236e976fd3fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prompts: 498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Round 2 responses: 100%|██████████| 498/498 [1:18:06<00:00,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 481 synthetic preference pairs for Round 2\n",
      " Synthetic preferences Round 2 saved to /content/drive/MyDrive/outputs/synthetic_preferences_round2.json\n",
      "\n",
      " Synthetic Data Generation Round 2 Complete!\n",
      "\n",
      "=== Round 2 Statistics ===\n",
      "Average chosen score: 2.4966\n",
      "Average rejected score: 0.3973\n",
      "Average score difference: 2.0993\n",
      "Min score difference: 0.1084\n",
      "Max score difference: 6.8633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"max length is {config.max_length}\")\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.dpo_round1_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load DPO Round 1 model (Base + LoRA adapters)\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.sft_model_dir,\n",
    "    torch_dtype=torch.float16,  # fp16 for Colab T4\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"Loading DPO Round 1 LoRA adapters...\")\n",
    "dpo_model = PeftModel.from_pretrained(base_model, config.dpo_round1_dir)\n",
    "dpo_model.eval()\n",
    "\n",
    "# Load reward model\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        self.reward_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        last_hidden = hidden_states[torch.arange(batch_size, device=hidden_states.device), sequence_lengths]\n",
    "        reward = self.reward_head(last_hidden)\n",
    "        return reward\n",
    "\n",
    "print(\"Loading reward model...\")\n",
    "reward_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.reward_model_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "reward_model = RewardModel(reward_base_model)\n",
    "checkpoint = torch.load(os.path.join(config.reward_model_dir, \"reward_model.pt\"))\n",
    "reward_model.reward_head.load_state_dict(checkpoint['reward_head_state_dict'])\n",
    "\n",
    "# Convert reward head to fp16\n",
    "reward_model.reward_head = reward_model.reward_head.to(torch.float16)\n",
    "reward_model = reward_model.to(device)\n",
    "reward_model.eval()\n",
    "\n",
    "print(\"Model dtype check:\")\n",
    "print(f\"DPO model dtype: {next(dpo_model.parameters()).dtype}\")\n",
    "print(f\"Reward head dtype: {next(reward_model.reward_head.parameters()).dtype}\")\n",
    "\n",
    "# Load NEW prompts (different from Round 1)\n",
    "print(\"\\nLoading prompts...\")\n",
    "dataset = load_dataset(config.dataset_name, split=\"train\")\n",
    "# Skip first 1000 used in Round 1, get next 500\n",
    "start_idx = 1000\n",
    "end_idx = start_idx + config.num_gen_samples\n",
    "dataset = dataset.select(range(start_idx, min(end_idx, len(dataset))))\n",
    "\n",
    "prompts = []\n",
    "for ex in dataset:\n",
    "    try:\n",
    "        prompt = ex['chosen'].split('Assistant:')[0].replace('Human:', '').strip()\n",
    "        if prompt and len(prompt) > 10:\n",
    "            prompts.append(prompt)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "prompts = prompts[:config.num_gen_samples]\n",
    "print(f\"Number of prompts: {len(prompts)}\")\n",
    "\n",
    "# Generate responses and score\n",
    "synthetic_data = []\n",
    "\n",
    "for prompt in tqdm(prompts, desc=\"Generating Round 2 responses\"):\n",
    "    responses = []\n",
    "\n",
    "    # Generate multiple responses\n",
    "    for _ in range(config.num_responses_per_prompt):\n",
    "        input_text = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=config.max_length).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = dpo_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,  # Reduced from 150 for faster generation\n",
    "                temperature=config.temperature,\n",
    "                top_p=config.top_p,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "        responses.append(response)\n",
    "\n",
    "    # Score responses with reward model\n",
    "    scores = []\n",
    "    for response in responses:\n",
    "        full_text = f\"Human: {prompt}\\n\\nAssistant: {response}\"\n",
    "        inputs = tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=config.max_length, padding=\"max_length\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            score = reward_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "\n",
    "        scores.append(score.item())\n",
    "\n",
    "    # Select chosen and rejected\n",
    "    max_idx = np.argmax(scores)\n",
    "    min_idx = np.argmin(scores)\n",
    "\n",
    "    if max_idx != min_idx and abs(scores[max_idx] - scores[min_idx]) > 0.1:\n",
    "        synthetic_data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"chosen\": responses[max_idx],\n",
    "            \"rejected\": responses[min_idx],\n",
    "            \"chosen_score\": float(scores[max_idx]),\n",
    "            \"rejected_score\": float(scores[min_idx])\n",
    "        })\n",
    "\n",
    "print(f\"\\nGenerated {len(synthetic_data)} synthetic preference pairs for Round 2\")\n",
    "\n",
    "# Save synthetic data\n",
    "with open(config.synthetic_data_path2, 'w') as f:\n",
    "    json.dump(synthetic_data, f, indent=2)\n",
    "\n",
    "print(f\" Synthetic preferences Round 2 saved to {config.synthetic_data_path2}\")\n",
    "print(\"\\n Synthetic Data Generation Round 2 Complete!\")\n",
    "\n",
    "# Print statistics\n",
    "if synthetic_data:\n",
    "    chosen_scores = [d['chosen_score'] for d in synthetic_data]\n",
    "    rejected_scores = [d['rejected_score'] for d in synthetic_data]\n",
    "    score_diffs = [d['chosen_score'] - d['rejected_score'] for d in synthetic_data]\n",
    "\n",
    "    print(\"\\n=== Round 2 Statistics ===\")\n",
    "    print(f\"Average chosen score: {np.mean(chosen_scores):.4f}\")\n",
    "    print(f\"Average rejected score: {np.mean(rejected_scores):.4f}\")\n",
    "    print(f\"Average score difference: {np.mean(score_diffs):.4f}\")\n",
    "    print(f\"Min score difference: {np.min(score_diffs):.4f}\")\n",
    "    print(f\"Max score difference: {np.max(score_diffs):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
