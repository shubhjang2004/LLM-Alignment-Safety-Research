{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24071,
     "status": "ok",
     "timestamp": 1761242809248,
     "user": {
      "displayName": "shivanimeena",
      "userId": "14777463049176070184"
     },
     "user_tz": -330
    },
    "id": "GQDV1gs6O5e0",
    "outputId": "377f9b64-6270-468b-b0fd-649234c82b20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13751,
     "status": "ok",
     "timestamp": 1761242835281,
     "user": {
      "displayName": "shivanimeena",
      "userId": "14777463049176070184"
     },
     "user_tz": -330
    },
    "id": "ILTL7wTOJzGn",
    "outputId": "95b954ed-c2b2-48b4-bdeb-0e0c3c36defc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Reward Model Training - Complete Code\n",
    "# Run this entire notebook AFTER SFT training\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1761243025275,
     "user": {
      "displayName": "shivanimeena",
      "userId": "14777463049176070184"
     },
     "user_tz": -330
    },
    "id": "C_PNG4d3J3NC",
    "outputId": "69b6fb17-26b6-4644-9006-35e95cc780fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓Config loaded!\n",
      "   Base model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "   SFT model path: /content/drive/MyDrive/outputs/sft_model\n",
      "   Reward model path: /content/drive/MyDrive/outputs/reward_model\n"
     ]
    }
   ],
   "source": [
    "# Configuration - FIXED for Reward Model Training\n",
    "class Config:\n",
    "    # Base model name (needed for tokenizer if SFT model doesn't exist)\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  #  ADD THIS\n",
    "\n",
    "    # Paths\n",
    "    sft_model_dir = \"/content/drive/MyDrive/outputs/sft_model\"  # Fixed typo: Mydrive → MyDrive\n",
    "    reward_model_dir = \"/content/drive/MyDrive/outputs/reward_model\"\n",
    "\n",
    "    # Data\n",
    "    dataset_name = \"Anthropic/hh-rlhf\"\n",
    "    num_preference_samples = 5000\n",
    "    max_length = 256  #  Changed from 512 to match your T4 setup\n",
    "\n",
    "    # Training\n",
    "    reward_epochs = 3\n",
    "    batch_size = 4\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.reward_model_dir, exist_ok=True)\n",
    "\n",
    "print(f\"✓Config loaded!\")\n",
    "print(f\"   Base model: {config.model_name}\")\n",
    "print(f\"   SFT model path: {config.sft_model_dir}\")\n",
    "print(f\"   Reward model path: {config.reward_model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667,
     "referenced_widgets": [
      "dcb545bd3ebb4a88859d95b7e7e2d4eb",
      "fe9356723af341a8866800f5202d789f",
      "cbd91baab82840fb89a28c86d87266ff",
      "2b89acf28034408796f0436924928dd0",
      "5676414c37dd4711b773413346c90d6e",
      "deb88dd2e91f43d8b8a21c79eb571e25",
      "551c3404021445e6b7e86a53f2350545",
      "9c017594da744f36a95b728543b1fb1f",
      "f801b000755c4a24b1bb3cb5bcb204ac",
      "e85b6e57809a4fed8aeb9f4795e3161f",
      "2a0aac7dd145405d8ac3de918d08b628",
      "5e0fe52410974d968c456509f5aefa99",
      "eccc2e3bd7704fc58aeb315472aeca16",
      "3c7bbd3bb10d4585af0e29294c70b309",
      "6e4429134318459f9e54707549413745",
      "343b0c7b68554735b28e3caea60c922e",
      "90277987a2a9494388fae5027c19b7df",
      "a361f87160954fbb85eccebfdbdf085b",
      "9491be91eeda4d3a895b8aa167b24552",
      "9b81a1c71a704931849817f6ff489d9a",
      "3e9766ab19b14ff2888ab38803efdccf",
      "c3ea3a2eb957444dbfaf78055ec00d54",
      "db33b8747e7a43d198e3996855a5a69e",
      "0d509ce4dd134eb2babd6ad67d555eb4",
      "3a197c48add547ada6be0f6cdd0bf577",
      "b90e6f2f2b1a4f46b64b732985699d7c",
      "b1181d7a09ae420599ced847ae979b76",
      "5c047386b8534ca4b2442164e1b0fa8d",
      "d215ca5896194d4b9613dcc6548e67a2",
      "370a076b646643b59461f683a7150777",
      "82be280098b64d39852c49d997eeb256",
      "d1d939decff94aa5bb1e0c41b1e1e150",
      "dabacf319f8d467bba93a51ae51c963d",
      "1fd741e78375490b8dca84344c66a236",
      "5115eb0e37be49e69ae8a3b134d6e750",
      "e8276a485e95431aafc4e5af0cb1c41d",
      "58a874e7ad37407cbcda3538752fb89d",
      "7cca3830e9a04bac9be439c493ca8828",
      "94189950df5842fbbfb778c782a725cc",
      "75e0f4619d3942428259f8f323bead59",
      "409a57e961d2447b9a35c181c967c7c9",
      "3000e271831b40d8b7e10c3ff3dda199",
      "50fc84b275274fd68b38f31bcf6c1975",
      "d1cd9d7ed9194462b8e12f56a729cb02",
      "0de20f0fa9b84cecb3d1c742e3c89029",
      "d952a3f308ad45019a5e7898816b4052",
      "6aaaca771f1542839f9917d3c81313c4",
      "d5625953a36a4d44ad98d5c476635a70",
      "51b6d12b113444d0af6fbc8388480981",
      "311bce1b4e94438ea53538bf83c7c87b",
      "5e45cca34e3147faa140487773bb0b0f",
      "c4dea1faf4fe4e76ba8d192bd019ea08",
      "86327d9ab69f40099f15582727c668eb",
      "4ef39dd69cc0417f91673347924c1f5a",
      "8660ead202e545e1ac3d413324730fea",
      "358c495bfadf40219e48269f2369b361",
      "83aa8cc585314fbca19f0f9bd68bdee2",
      "8ea1901007f5421098513656ada7fd6a",
      "f14277fb45004655838d5a8deddb2fc5",
      "d4b2efdfbd754ce1b62ed61a8bc25d29",
      "58f2ab227f034296bb4feb1778cd92da",
      "bcb444359058456499ffb4700ff2d17d",
      "eaa351aa877a4c8eb209654c33b6e924",
      "d32a2c46fdee440cb0fe17a624326d38",
      "10dd28d557924b61bb23e3b56522e283",
      "40244cde41c64b72b37b7901f103562e",
      "81d9a9f52a9e416488283d20a4dd0554",
      "7d61f8c6fc164d4b87da4b0161f04f93",
      "48cca0d6fc96486bb3514fe8b8ec2e2b",
      "612de1dd61ca44ecb3f1fffbbcfb48c7",
      "59a41f4b71e147128487c9744cbe7e69",
      "efee5be34c5744149909adb95861ab08",
      "28f7d8221fa0443daf0f188b9d46f900",
      "541af4da4bae40929a92c71b4ebe68b1",
      "a60ee9fb013f48b1bd9552c69fe49b7c",
      "9c726f1938c34b6488395fcb31be023a",
      "cba881a68edf4be9bd7f84bfcd07ddbb",
      "a62857255cc94781a733f5aa83a22d29",
      "ee7298a877f245068a4d9678ddf7d3d3",
      "35c530e907664530a341483a2893fc26",
      "4632348843154654a5e2901f760f2eae",
      "f645a868371641e1852442657a9980f6",
      "d63456a836d94eeebfb48ada12d79c02",
      "b6b4c376960c40f1a283ca851f9bef70",
      "5c00ed701ea34a638ccd96858c235164",
      "6ea2d7afa97b4f88911b0166b766b639",
      "ed501d17c7494cff875a2f531ae13e3e",
      "c285e3d8161647c4b2ddee92f013ac48",
      "8030075074b444e6971e20985055f6dd",
      "71370d460e0d4538a572833a6e7b16df",
      "46eb8dacd2a64ec3b74204d91f061baf",
      "52a13c1698554e8eabf2e2757a233cb5",
      "c3e98ff0c9284b3da3a24421600de413",
      "84042276ace34b90aba1ba3954da344e",
      "313853314d3e4cfbb6d86d5f5c55fe75",
      "76a055134ea84ba1b24d96cf8b782546",
      "d259c27efe774f3fa6aeefa6a94396ce",
      "cb43711cf1634159b1b58a309ae387aa",
      "cd8c0838758d482c8d1717d60a5a5f38",
      "d14cd4505f61469289a34997a508241a",
      "4a016c35d3c64eb58650beaf73ef9c33",
      "ef3a67f262b14fc5965e894dd564c12e",
      "d8197b2f89754afdaa71e62ee72181c7",
      "e7ae574f09c94bdf9a9a839e756e8316",
      "04e2cb6cade54a498469eab11af3d034",
      "753670a768b547ac8eafa914d9766b92",
      "4ad1d3a2f3af49c9b8fd4e413083d4ce",
      "70f0cca7bf694d9497aa5f0615260664",
      "a44c6a1a87554396a486c8ffc79eac35",
      "1f24be8b80c34719b0a0709a480b88b9",
      "32708c31a18f4eb7bb6bd45568d452fb",
      "8130ffc9d0e54fb9b14bc3c838cb3608",
      "b5bc77cebac24f6d80f04d3fadd2b00a",
      "d7c5255c9a9f4b55827729a2c2f32301",
      "6fc89fdf681944e6b4b464ca69a819f8",
      "8fc277b8f3bc4b079de52607ff7f162e",
      "554c52c7aa3c4a488a9507c5e1643015",
      "b178b930836b4368b908ac22b1ba86b8",
      "c6eca7e8d38548f592911e8d4f9bf84f",
      "20ee1918005848129c9d31d2367a396b",
      "39590080b40c4ea4939bbf849c1d1546"
     ]
    },
    "executionInfo": {
     "elapsed": 12989,
     "status": "ok",
     "timestamp": 1761243112854,
     "user": {
      "displayName": "shivanimeena",
      "userId": "14777463049176070184"
     },
     "user_tz": -330
    },
    "id": "jabWs9-LJ53b",
    "outputId": "ae0d226e-66f7-479d-c8fb-17ab6ddcd46a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "   Loading from saved model: /content/drive/MyDrive/outputs/sft_model\n",
      "  Tokenizer loaded successfully!\n",
      "   Vocab size: 32000\n",
      "   Pad token: </s>\n",
      "\n",
      "Loading preference data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb545bd3ebb4a88859d95b7e7e2d4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0fe52410974d968c456509f5aefa99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "harmless-base/train.jsonl.gz:   0%|          | 0.00/13.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db33b8747e7a43d198e3996855a5a69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-base/train.jsonl.gz:   0%|          | 0.00/16.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd741e78375490b8dca84344c66a236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-online/train.jsonl.gz:   0%|          | 0.00/20.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de20f0fa9b84cecb3d1c742e3c89029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-rejection-sampled/train.jsonl.gz:   0%|          | 0.00/25.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358c495bfadf40219e48269f2369b361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "harmless-base/test.jsonl.gz:   0%|          | 0.00/743k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d9a9f52a9e416488283d20a4dd0554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-base/test.jsonl.gz:   0%|          | 0.00/875k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62857255cc94781a733f5aa83a22d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-online/test.jsonl.gz:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8030075074b444e6971e20985055f6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "helpful-rejection-sampled/test.jsonl.gz:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14cd4505f61469289a34997a508241a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/160800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32708c31a18f4eb7bb6bd45568d452fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formatting preferences: 100%|██████████| 5000/5000 [00:00<00:00, 32594.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preference dataset size: 4999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL: Load Tokenizer and Preference Data (COMPLETE)\n",
    "# ..........................................\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "\n",
    "# Check if we have a trained model saved\n",
    "if os.path.exists(config.sft_model_dir) and os.path.exists(os.path.join(config.sft_model_dir, \"tokenizer_config.json\")):\n",
    "    # Load from saved model\n",
    "    print(f\"   Loading from saved model: {config.sft_model_dir}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.sft_model_dir, trust_remote_code=True)\n",
    "else:\n",
    "    # Load from base model\n",
    "    print(f\"   Loading from base model: {config.model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name, trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"  Tokenizer loaded successfully!\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "print(f\"   Pad token: {tokenizer.pad_token}\")\n",
    "\n",
    "# Load preference data\n",
    "print(\"\\nLoading preference data...\")\n",
    "dataset = load_dataset(config.dataset_name, split=\"train\")\n",
    "pref_data = dataset.select(range(min(config.num_preference_samples, len(dataset))))\n",
    "\n",
    "def format_preference(example):\n",
    "    try:\n",
    "        prompt = example['chosen'].split('Assistant:')[0].replace('Human:', '').strip()\n",
    "        chosen = example['chosen'].split('Assistant:')[-1].strip()\n",
    "        rejected = example['rejected'].split('Assistant:')[-1].strip()\n",
    "        return {\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "preference_data = []\n",
    "for ex in tqdm(pref_data, desc=\"Formatting preferences\"):\n",
    "    formatted = format_preference(ex)\n",
    "    if formatted and formatted['prompt'] and formatted['chosen'] and formatted['rejected']:\n",
    "        preference_data.append(formatted)\n",
    "\n",
    "print(f\"\\nPreference dataset size: {len(preference_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1761243118489,
     "user": {
      "displayName": "shivanimeena",
      "userId": "14777463049176070184"
     },
     "user_tz": -330
    },
    "id": "yWNQaTaBJ7-i"
   },
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class RewardDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        chosen_text = f\"Human: {item['prompt']}\\n\\nAssistant: {item['chosen']}\"\n",
    "        rejected_text = f\"Human: {item['prompt']}\\n\\nAssistant: {item['rejected']}\"\n",
    "\n",
    "        chosen_tokens = self.tokenizer(\n",
    "            chosen_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        rejected_tokens = self.tokenizer(\n",
    "            rejected_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"chosen_input_ids\": chosen_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"chosen_attention_mask\": chosen_tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"rejected_input_ids\": rejected_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"rejected_attention_mask\": rejected_tokens[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "train_dataset = RewardDataset(preference_data, tokenizer, config.max_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1761243120708,
     "user": {
      "displayName": "shivanimeena",
      "userId": "14777463049176070184"
     },
     "user_tz": -330
    },
    "id": "7IMkcVwaKCJG"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reward Model Architecture\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        self.reward_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        last_hidden = hidden_states[torch.arange(batch_size, device=hidden_states.device), sequence_lengths]\n",
    "\n",
    "        reward = self.reward_head(last_hidden)\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411,
     "referenced_widgets": [
      "dd387aad8f064c289d060d8ba0559d34",
      "d45c6cf8a8d34738b3b430f5597260d7",
      "8db992ce49ab418ab5f825422714ef6f",
      "c1b1ed4c891b4380b5c475e147ab1ad8",
      "e8a4962114fe4f33ad9bc27741ee6f09",
      "4da744d8e9784ca19702a4b9838b52d7",
      "14b6e44f2b40479fad7f39a4a95ae858",
      "4d3b085800e94cac8a723629ff5dbc43",
      "e87ac634d195482d984aecc59f18f188",
      "fe9bf3c092144df2a234dc00b0359037",
      "ab50322c7c4e4ffaba6725c30c5b1dd6",
      "2e6d1df90c9e42cfb69888a47e11ca50",
      "de47e44f64854cb2b5f1d5d93f234609",
      "ae2d5c454b7e46cb8372b71baaeb769d",
      "ea9770f623274e608190856e24d2447f",
      "a9c7ada77cd04c4dbcd956eb1c01052a",
      "69d3cbb0f1f64e13ba46480d08f0b685",
      "412d0078d4604746afeefc8403285e0a",
      "86e949cf5327403a81f2a88122154ee8",
      "18bb44f6dc2345b7a4cc0eabfc9feb34",
      "92933eaf4b5d40f190ca4ec85736e3c7",
      "5bd86c760aa94fa683e43bbee8780599",
      "282124b48e7a4a35adfeb0e1e766196f",
      "4c1bf5c0bf4e49c9ae464e304b04996e",
      "4cfa45f1c9484b80bb95bd29cb23f775",
      "52cdbff86f744118896df1b354b31ca1",
      "14f1b1f5fb5d4c4483476232a33f3d9d",
      "f5e19da3ec05463ab158709874e2349d",
      "33743ec257404fc9955429597d98c2f0",
      "a81dba3d5a5843f794662bab2945e2b9",
      "d5a3aa4f90f74fa49a7c22aecf75259e",
      "3379649e95f94faba656acc190c97135",
      "b88517f4b7b641d19bc9bcde979e484a"
     ]
    },
    "executionInfo": {
     "elapsed": 1117309,
     "status": "ok",
     "timestamp": 1761244241373,
     "user": {
      "displayName": "shivanimeena",
      "userId": "14777463049176070184"
     },
     "user_tz": -330
    },
    "id": "bVZXmh0uJV0d",
    "outputId": "cde1b583-d64e-4726-89c5-ec8d0ea8c89a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SFT model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd387aad8f064c289d060d8ba0559d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6d1df90c9e42cfb69888a47e11ca50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282124b48e7a4a35adfeb0e1e766196f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating reward model...\n",
      "Starting reward model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/3:   0%|          | 0/1250 [00:00<?, ?it/s]/tmp/ipython-input-3648423269.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
      "Epoch 1/3: 100%|██████████| 1250/1250 [06:02<00:00,  3.44it/s, loss=0.498, acc=0.667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.6697, Accuracy: 0.6309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1250/1250 [05:59<00:00,  3.48it/s, loss=0.439, acc=0.667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.6035, Accuracy: 0.6829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1250/1250 [05:55<00:00,  3.52it/s, loss=0.595, acc=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 0.5691, Accuracy: 0.7077\n",
      "Saving reward model...\n",
      " Reward model saved to /content/drive/MyDrive/outputs/reward_model\n",
      "\n",
      " Reward Model Training Complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load base model\n",
    "print(\"Loading SFT model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.sft_model_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Create reward model\n",
    "print(\"Creating reward model...\")\n",
    "reward_model = RewardModel(base_model)\n",
    "reward_model = reward_model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(reward_model.reward_head.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting reward model training...\")\n",
    "reward_model.train()\n",
    "\n",
    "for epoch in range(config.reward_epochs):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.reward_epochs}\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        chosen_ids = batch[\"chosen_input_ids\"].to(device)\n",
    "        chosen_mask = batch[\"chosen_attention_mask\"].to(device)\n",
    "        rejected_ids = batch[\"rejected_input_ids\"].to(device)\n",
    "        rejected_mask = batch[\"rejected_attention_mask\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            chosen_reward = reward_model(chosen_ids, chosen_mask)\n",
    "            rejected_reward = reward_model(rejected_ids, rejected_mask)\n",
    "\n",
    "            loss = -torch.log(torch.sigmoid(chosen_reward - rejected_reward)).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(reward_model.reward_head.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        acc = (chosen_reward > rejected_reward).float().mean().item()\n",
    "        total_acc += acc\n",
    "\n",
    "        pbar.set_postfix({\"loss\": loss.item(), \"acc\": acc})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_acc = total_acc / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "# Save reward model\n",
    "print(\"Saving reward model...\")\n",
    "torch.save({\n",
    "    'reward_head_state_dict': reward_model.reward_head.state_dict(),\n",
    "    'config': config.__dict__\n",
    "}, os.path.join(config.reward_model_dir, \"reward_model.pt\"))\n",
    "\n",
    "base_model.save_pretrained(config.reward_model_dir)\n",
    "tokenizer.save_pretrained(config.reward_model_dir)\n",
    "\n",
    "print(f\" Reward model saved to {config.reward_model_dir}\")\n",
    "print(\"\\n Reward Model Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP4nSefnMkAChDDlUQqFYR9",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
